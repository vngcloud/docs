# Model Catalog

AI Platform h·ªó tr·ª£ ng∆∞·ªùi d√πng tri·ªÉn khai nhanh c√°c Large Language Model (LLM) v√† Embeding Model ph·ªï bi·∫øn th√¥ng qua **Model Catalog**. M·ªói model y√™u c·∫ßu c·∫•u h√¨nh ph·∫ßn c·ª©ng ph√π h·ª£p ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªáu nƒÉng v√† ƒë·ªô ·ªïn ƒë·ªãnh khi ph·ª•c v·ª• inference.

## Quy tr√¨nh Inference v·ªõi Model t·ª´ Catalog

#### **B∆∞·ªõc 1: Import Model t·ª´ Catalog v√†o Model Registry**

* **M·ª•c ƒë√≠ch**: ƒêƒÉng k√Ω model v√†o h·ªá th·ªëng ƒë·ªÉ c√≥ th·ªÉ d√πng l√†m ƒë·∫ßu v√†o cho Inference Endpoint.
* **H√†nh ƒë·ªông c·∫ßn l√†m**:
  * Truy c·∫≠p tab **Model Registry** trong giao di·ªán AI Platform.
  * Ch·ªçn **Import t·ª´ AI Platform Catalog**.
  * T√¨m ki·∫øm model mong mu·ªën (v√≠ d·ª•: `meta-llama/Meta-Llama-3-8B-Instruct`).

> Sau khi import th√†nh c√¥ng, model s·∫Ω xu·∫•t hi·ªán trong danh s√°ch Model Registry v√† c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ tri·ªÉn khai endpoint.

#### **B∆∞·ªõc 2: Kh·ªüi t·∫°o Inference Endpoint**

* **M·ª•c ƒë√≠ch**: T·∫°o m·ªôt endpoint ƒë·ªÉ g·ª≠i request inference ƒë·∫øn model v·ª´a import.
* **H√†nh ƒë·ªông c·∫ßn l√†m**:
  * Truy c·∫≠p tab **Inference** ‚Üí Ch·ªçn **Create Endpoint**.
  * ƒêi·ªÅn th√¥ng tin:
    * **Endpoint Name**
    * **Region** (v√≠ d·ª•: HCM)
    * **Ch·ªçn Model Registry** ƒë√£ import ·ªü b∆∞·ªõc tr∆∞·ªõc
    * **Ch·ªçn Resource Configuration**: lo·∫°i GPU (g1-standard-4x16-1rtx2080ti, v.v.), dung l∆∞·ª£ng RAM, CPU ph√π h·ª£p
    * **Replica Configuration**: s·ªë l∆∞·ª£ng t·ªëi thi·ªÉu v√† t·ªëi ƒëa b·∫£n sao (ƒë·ªÉ auto-scaling)
  * Nh·∫•n **Create Endpoint**

> Sau v√†i ph√∫t kh·ªüi t·∫°o, b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c m·ªôt endpoint s·∫µn s√†ng ph·ª•c v·ª• inference th√¥ng qua API.

## Y√™u C·∫ßu Ph·∫ßn C·ª©ng

AI Platform h·ªó tr·ª£ ng∆∞·ªùi d√πng tri·ªÉn khai nhanh c√°c Large Language Model (LLM) v√† Embeding Model ph·ªï bi·∫øn th√¥ng qua **Model Catalog**. M·ªói model y√™u c·∫ßu c·∫•u h√¨nh ph·∫ßn c·ª©ng ph√π h·ª£p ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªáu nƒÉng v√† ƒë·ªô ·ªïn ƒë·ªãnh khi ph·ª•c v·ª• inference.

### LLM Models

<table data-header-hidden><thead><tr><th width="122"></th><th width="109"></th><th width="106"></th><th width="97"></th><th width="229"></th><th></th></tr></thead><tbody><tr><td><strong>Model</strong></td><td><strong>RTX 2080</strong></td><td><strong>RTX 4090</strong></td><td><strong>A40</strong></td><td><strong>Note</strong></td><td><strong>Th·ªùi gian t·∫°o (ph√∫t) v·ªõi RTX 2080</strong></td></tr><tr><td>google/gemma-2-9b-it</td><td>3 card</td><td>1 card</td><td>1 card</td><td>3 card rtx 2080 c·∫ßn gi·∫£m max context length &#x3C;= 22624</td><td>48</td></tr><tr><td>microsoft/Phi-3-medium-4k-instruct</td><td>4 card</td><td>2 card</td><td>1 card</td><td></td><td>32</td></tr><tr><td>Qwen/Qwen2.5-7B-Instruct</td><td>2 card</td><td>1 card</td><td>1 card</td><td></td><td>27</td></tr><tr><td>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</td><td>2 card</td><td>1 card</td><td>1 card</td><td></td><td>24</td></tr><tr><td>deepseek-ai/DeepSeek-R1-Distill-Qwen-14B</td><td>3 card</td><td>2 card</td><td>1 card</td><td></td><td>24</td></tr><tr><td>Qwen/Qwen2.5-14B-Instruct</td><td>4 card</td><td>2 card</td><td>1 card</td><td>4 card rtx 2080 c·∫ßn max context length &#x3C;= 8960 &#x26;&#x26; max_num_seqs &#x3C;= 256</td><td>24</td></tr><tr><td>meta-llama/Llama-3.2-3B</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>22</td></tr><tr><td>meta-llama/Meta-Llama-3-8B-Instruct</td><td>2 card</td><td>1 card</td><td>1 card</td><td></td><td>21</td></tr><tr><td>meta-llama/Meta-Llama-3-8B</td><td>2 card</td><td>1 card</td><td>1 card</td><td></td><td>20</td></tr><tr><td>deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</td><td>2 card</td><td>1 card</td><td>1 card</td><td>2 card rtx 2080 c·∫ßn gi·∫£m max context length &#x3C;= 8192 &#x26;&#x26; max_num_seqs&#x3C;=256</td><td>19</td></tr><tr><td>Qwen/Qwen2.5-0.5B-Instruct</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>14</td></tr><tr><td>meta-llama/Llama-3.2-3B-Instruct</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>17</td></tr><tr><td>google/gemma-2-2b-it</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>16</td></tr><tr><td>meta-llama/Llama-3.2-1B</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>16</td></tr><tr><td>microsoft/Phi-3.5-mini-instruct</td><td>1 card</td><td>1 card</td><td>1 card</td><td>1 card rtx 2080 c·∫ßn gi·∫£m max context length &#x3C;= 18080</td><td>16</td></tr><tr><td>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>15</td></tr><tr><td>meta-llama/Llama-3.2-1B-Instruct</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>15</td></tr><tr><td>Qwen/Qwen2.5-1.5B-Instruct</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>15</td></tr><tr><td>SeaLLMs/SeaLLMs-v3-1.5B</td><td>1 card</td><td>1 card</td><td>1 card</td><td></td><td>17</td></tr><tr><td>SeaLLMs/SeaLLMs-v3-7B</td><td>2 card</td><td>1 card</td><td>1 card</td><td>2x2080 c·∫ßn gi·∫£m Max context length &#x3C;= 37984</td><td>37</td></tr></tbody></table>

### Embedding Models

|                                                             |              |              |         |                                |
| ----------------------------------------------------------- | ------------ | ------------ | ------- | ------------------------------ |
| **Model**                                                   | **RTX 2080** | **RTX 4090** | **A40** | **Th·ªùi gian t·∫°o v·ªõi RTX 2080** |
| sentence-transformers/all-MiniLM-L6-v2                      | 1 card       | 1 card       | 1 card  | 15 ph√∫t                        |
| BAAI/bge-m3                                                 | 1 card       | 1 card       | 1 card  | 16 ph√∫t                        |
| intfloat/multilingual-e5-large-instruct                     | 1 card       | 1 card       | 1 card  | 14 ph√∫t                        |
| sentence-transformers/paraphrase-multilingual-mpnet-base-v2 | 1 card       | 1 card       | 1 card  | 14 ph√∫t                        |

{% hint style="info" %}
#### **L∆∞u √Ω c·∫•u h√¨nh VRAM**

B·∫°n c√≥ th·ªÉ tra c·ª©u m·ª©c s·ª≠ d·ª•ng VRAM c·ªßa t·ª´ng model t·∫°i:\
üîó [https://llm.extractum.io](https://llm.extractum.io)
{% endhint %}

| GPU      | VRAM  |
| -------- | ----- |
| RTX 2080 | 10 GB |
| RTX 4090 | 24 GB |
| A40      | 46 GB |
